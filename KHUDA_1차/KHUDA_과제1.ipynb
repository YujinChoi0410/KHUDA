{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쿠다 과제 1\n",
    "\n",
    "- **임의의 영문 기사를 스크랩**하여 해당 본문에서 나타나는 단어들의 유사도 및 문맥&타깃의 정답 확률을 파악해보는 과제입니다.\n",
    "- 각 단계별로 유사도 측정에서 어떤 차이가 있는지 비교 분석 해볼 수 있습니다.\n",
    "- 추론 기반 기법에서 문맥을 통해 타깃을 얼마나 정확하게 맞출 수 있는지 파악해볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시 기사 본문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nasdaq.com/articles/what-are-consensus-estimates-and-why-are-they-important\n",
    "\n",
    "Why do stocks go up?\n",
    "\n",
    "There are a number of reasons.\n",
    "\n",
    "One is this: a stock often goes up if it beats expectations.\n",
    "\n",
    "By expectations, we don’t mean in a general sense, but in a quantifiable sense, i.e. Consensus. Consensus is, essentially, the average of earnings estimates made by professionals.\n",
    "\n",
    "You may notice that occasionally articles in the financial press refer to earnings estimates. Publishing earnings estimates is one of the functions of analysts, whose job is to research stocks. They are employed by companies such as J.P. Morgan, Goldman Sachs, Barclays, and many others and cater mostly to large, institutional investors.\n",
    "\n",
    "Putting Consensus into Practice\n",
    "Let’s start with an example and take a look at Apple (AAPL).\n",
    "\n",
    "AAPL consensus\n",
    "That refers to the average of analyst earnings estimates for the particular quarter we are in. Notice that the number is $1.00. This number will likely fluctuate as analysts adjust their numbers as the earnings date approaches.\n",
    "\n",
    "If you are a trader, you may seek to determine whether Apple will beat or miss that $1.00. You can do that by reading research reports, following the news, reviewing management appearances, and if possible looking into scuttlebutt. You would consider laterals. And you will distill all that information into a viewpoint on earnings — perhaps Apple will make $1.05 in the quarter. Maybe you’ve found evidence that sales for laptops are higher than expected. Or perhaps you conclude that Apple will make $0.90 instead because of chip shortages.\n",
    "\n",
    "And if you do determine that Apple will make $1.05, and Consensus remains at $1.00, all things equal, Apple stock may go up if you’re correct. However, there are exogenous factors. What if inflation skyrockets? What if interest rates rise?\n",
    "\n",
    "Important Considerations\n",
    "A company beating Consensus estimates does not *guarantee* that their stock will go up. There are times when a company will beat the Consensus earnings estimate, but its stock goes down.\n",
    "\n",
    "That may be because the Consensus estimate may not be a “true” reflection of the actual expectations of all market participants. Analyst estimates can be stale, so what’s listed as Consensus may be higher (or lower) than it should be. For example, Consensus for Apple may be higher than it should be because not all analysts have adjusted their estimates downward for chip shortages.\n",
    "\n",
    "Moreover, a company may beat quarterly Consensus and its stock go down because of forward-looking comments that miss expectations. To go back to Apple, the company may beat the current quarter, but provide comments that suggest they will miss expectations for the following quarter, and, as a consequence, the stock goes down. If a trader anticipated this scenario, they may want to trim their position.\n",
    "\n",
    "While one should pay close attention to quarterly earnings reports, Consensus is also applicable to a longer-term, investor-oriented mindset. You may have noticed in the company profile “Fwd EPS (Curr Yr).” That pertains to this year’s Consensus earnings estimate, which is $5.16. Maybe you don’t care about the chip shortages temporarily impacting the stock and subscribe to a longer-term view that Apple’s on-going revenue shift toward software and services is not fully appreciated, and that will drive earnings beats over time. That could manifest in Apple exceeding $5.16 in earnings this year, as well as next year’s expectations.\n",
    "\n",
    "The bottom line? Whether you decide to undertake short-term trades or invest with a long-term mindset, Consensus is a useful framework for understanding stock price dynamics.\n",
    "\n",
    "Originally published on Tornado.com: What are Consensus Estimates and Why are they Important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리\n",
    " - NLTK를 활용해 스크랩한 기사 본문을 전처리 해야합니다. \n",
    " - 앞으로 항상 데이터 전처리 과정은 기본이 될 것입니다.\n",
    " - 데이터 분석, AI, NLP에서 전처리에 소모되는 시간이 70%가 넘습니다.\n",
    " - 전처리 절차 전 python 가상환경을 미리 구축해 추후 실습에 지장이 없도록 합니다.\n",
    " - 구글 코랩을 사용해도 무방합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 기사에서 가져온 본문을 토큰화, 불용어 제거, 일정 길이 이하의 단어 제거, 소문자화, 표제어 추출, 어간 추출 등의 과정을 거칩니다.\n",
    "\n",
    "https://blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=qbxlvnf11&logNo=221434157182 (출처)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing (제공됨)\n",
    "def preprocessing(text):\n",
    "    # tokenize into words\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text)\n",
    "              for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    print( \"- tokenize into words -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # remove stopwords\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop]\n",
    "\n",
    "    print( \"- remove stopwords -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # remove words less than three letters\n",
    "    tokens = [word for word in tokens if len(word) >= 3]\n",
    "\n",
    "    print( \"- remove words less than three letters -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # lower capitalization\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "    print( \"- lower capitalization -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "    \n",
    "    # lemmatization\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tokens = [lmtzr.lemmatize(word) for word in tokens]\n",
    "\n",
    "    print( \"- lemmatization -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "\n",
    "    tokens = [lmtzr.lemmatize(word, 'v') for word in tokens]\n",
    "\n",
    "    print( \"- lemmatization/verb -\" )\n",
    "    print( tokens )\n",
    "    print()\n",
    "\n",
    "    # stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [ stemmer.stem(word) for word in tokens ]\n",
    "\n",
    "    print( \"- stemming -\" )\n",
    "    print(tokens)\n",
    "    print()\n",
    "    \n",
    "    preprocessed_text= ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파일 입출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 입출력\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 전처리 완료된 기사 본문을 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 완료된 기사 본문\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시소러스\n",
    "\n",
    "- NLTK를 활용해서 WordNet 시소러스를 통해 단어 유사도를 확인해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 2번에 수행한 변수를 리스트로 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 리스트로 쪼개기\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 본문 중 관계가 궁금한 단어 2개를 임의로 골라 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 두 단어 사이의 NLTK 기준 유사도를 구하기\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python import\n",
    "\n",
    "**아래 셀부터는 common 및 model을 import 함으로써 함수들을 분리하겠습니다.\n",
    "코드 작성에 참고 바랍니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계 기반 기법\n",
    "\n",
    "- 통계 기반 기법을 활용해 코사인 유사도를 확인합니다.\n",
    "- 성능을 개선하기 전과 후(PPMI, SVD)로 비교해 어떤 차이가 있는지 비교합니다.\n",
    "- 시소러스의 유사도와 같은 값을 지향하고 있는지 비교합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. create_co_matrix 함수를 util.py에 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 부모 디렉토리도 포함\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "window_size = 1\n",
    "vocab_size = len(id_to_word)\n",
    "\n",
    "C = create_co_matrix(corpus, vocab_size, window_size=1) # 해당 함수를 common.util.py에 만드세요.\n",
    "\n",
    "c0 = C[word_to_id['stock']]  # \"you\"의 단어 벡터\n",
    "c1 = C[word_to_id['earn']]  # 'i'의 단어 벡터\n",
    "print(\"두 단어 사이의 코사인 유사도\", cos_similarity(c0, c1)) # 해당 함수를 common.util.py에 만드세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. most_similar 함수를 만들고 util.py에 복사하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 비슷한 단어를 찾는 함수\n",
    "# 구현해보고 common/util.py에 복사합니다.\n",
    "\n",
    "# Code\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 6에서 만든 함수로 기사 본문에 등장하는 임의의 단어가 어떤 단어와 비슷한지 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 단어가 어떤 단어와 비슷한지 직접 경험해봅시다.\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ppmi 함수를 util.py에 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C) # 해당 함수를 common.util.py에 만드세요.\n",
    "\n",
    "np.set_printoptions(precision=3)  # 유효 자릿수를 세 자리로 표시\n",
    "print('Co-occurrence Matrix')\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.util import most_similar\n",
    "most_similar('you', word_to_id, id_to_word, W, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_method_small.py\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "# SVD\n",
    "U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 행렬의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C[0])  # 동시발생 행렬\n",
    "print(W[0])  # PPMI 행렬\n",
    "print(U[0])  # SVD\n",
    "# 2차원으로 차원 축소하기\n",
    "print(U[0, :2])\n",
    "\n",
    "# 플롯\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론 기반 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW 모델\n",
    "\n",
    "- CBOW 모델을 구현하기 위해 맥락, 타깃을 설정하고 One-Hot 인코딩을 수행합니다.\n",
    "- 구현한 모델을 평가합니다.\n",
    "- 만약 성능이 좋지 않다면 어떻게 개선해야할 지 생각해보고 해당 행위를 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. convert_one_hot 함수를 만들고 util.py에도 복사하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    \n",
    "# 해당 함수를 만들고 util.py에도 복사하세요.\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. create_contexts_target 함수를 util.py에 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "contexts, target = create_contexts_target(corpus, window_size=1) # 해당 함수를 common.util.py에 만드세요.\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. simple_cbow.py에 SimpleCBOW Class를 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap03/train.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.trainer import Trainer # 제공합니다.\n",
    "from common.optimizer import Adam # 제공합니다.\n",
    "from simple_cbow import SimpleCBOW # simple_cbow.py에 해당 클래스를 만드세요.\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id) # cbow 학습 데이터셋 생성\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size) # Input에 맞는 one-hot 표현 변환\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. 모델에서 저장한 Word Embedding의 입력층 값 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding 살펴보기 simple_cobw의 word_vec 참고\n",
    "# W_in 입니다.\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. 모델에서 저장한 Word Embedding의 출력층 값 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding 살펴보기, simple_cobw의 word_vec 참고\n",
    "# W_out 입니다.\n",
    "\n",
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# 100개의 단어에 대해서만 시각화\n",
    "X_tsne = tsne.fit_transform(word_vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(id_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(20, 10)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram 모델\n",
    "\n",
    "- CBOW 모델 구현에 성공했으면 skip-gram 모델로도 평가합니다. 둘의 성능이 어떻게 다른지 비교해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. simple_skip_gram.py에 SimpleSkipGram Class 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chap03/train.py\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_skip_gram import SimpleSkipGram # 해당 클래스를 구현하세요.\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(preprocessed)\n",
    "vocab_size = len(word_to_id) # cbow 학습 데이터셋 생성\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size) # Input에 맞는 one-hot 표현 변환\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "# 모델 초기화\n",
    "model = SimpleSkipGram(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding 살펴보기\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "# 100개의 단어에 대해서만 시각화\n",
    "X_tsne = tsne.fit_transform(word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(id_to_word.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(20, 10)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos, fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과제가 끝났습니다. 해당 모델을 이용해 다른 것들을 해보시고 싶으신 분들은 자유롭게 구현해주시면 되겠습니다.\n",
    "\n",
    "수고하셨습니다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c73e5749eef24712aa57a6f7056eff5de2d1cfcc1b2800305556963bdb3c96d5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
